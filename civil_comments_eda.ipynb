{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deeksha_1821cs13/anaconda3/envs/pssp/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_from_disk\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_from_disk(\"/Data/deeksha/disha/code_p/style_transformer_repl/civil_comments_ds\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "civil_comments = load_dataset('civil_comments', split='train')\n",
    "# civil_comments = civil_comments.select(range(1000))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_len(example):\n",
    "    print(example)\n",
    "    return {\"Length\":len(example['text'].split())}\n",
    "\n",
    "civil_comments.set_format('torch')\n",
    "civil_comments = civil_comments.map(avg_len)\n",
    "civil_comments.set_format(type=\"pandas\")\n",
    "civil_comments['Length'].plot.hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Each sentence has a length less than 512 tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset civil_comments (/Data/deeksha/.cache/huggingface/datasets/civil_comments/default/0.9.0/e7a3aacd2ab7d135fa958e7209d10b1fa03807d44c486e3c34897aa08ea8ffab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e918b8b357724bc49b2549132e94a644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit'],\n",
      "        num_rows: 1804874\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit'],\n",
      "        num_rows: 97320\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit'],\n",
      "        num_rows: 97320\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "# Load the Civil Comments dataset\n",
    "dataset = load_dataset(\"civil_comments\")\n",
    "# dataset = load_from_disk(\"./civil_comments_ds/\")\n",
    "\n",
    "# Print information about the dataset\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Data/deeksha/.cache/huggingface/datasets/civil_comments/default/0.9.0/e7a3aacd2ab7d135fa958e7209d10b1fa03807d44c486e3c34897aa08ea8ffab/cache-efbe845cfd4acd8b.arrow\n",
      "Loading cached processed dataset at /Data/deeksha/.cache/huggingface/datasets/civil_comments/default/0.9.0/e7a3aacd2ab7d135fa958e7209d10b1fa03807d44c486e3c34897aa08ea8ffab/cache-41376e979e9b3635.arrow\n",
      "Loading cached processed dataset at /Data/deeksha/.cache/huggingface/datasets/civil_comments/default/0.9.0/e7a3aacd2ab7d135fa958e7209d10b1fa03807d44c486e3c34897aa08ea8ffab/cache-da44b2f26f571be5.arrow\n",
      "Loading cached processed dataset at /Data/deeksha/.cache/huggingface/datasets/civil_comments/default/0.9.0/e7a3aacd2ab7d135fa958e7209d10b1fa03807d44c486e3c34897aa08ea8ffab/cache-60faabb33c4dbfc6.arrow\n",
      "Loading cached processed dataset at /Data/deeksha/.cache/huggingface/datasets/civil_comments/default/0.9.0/e7a3aacd2ab7d135fa958e7209d10b1fa03807d44c486e3c34897aa08ea8ffab/cache-f66b7dcd083cfb65.arrow\n",
      "Loading cached processed dataset at /Data/deeksha/.cache/huggingface/datasets/civil_comments/default/0.9.0/e7a3aacd2ab7d135fa958e7209d10b1fa03807d44c486e3c34897aa08ea8ffab/cache-3e593febe606425a.arrow\n",
      "Loading cached processed dataset at /Data/deeksha/.cache/huggingface/datasets/civil_comments/default/0.9.0/e7a3aacd2ab7d135fa958e7209d10b1fa03807d44c486e3c34897aa08ea8ffab/cache-00956642ccb2e209.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Data/deeksha/.cache/huggingface/datasets/civil_comments/default/0.9.0/e7a3aacd2ab7d135fa958e7209d10b1fa03807d44c486e3c34897aa08ea8ffab/cache-45ed8fdf14df59c3.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit'],\n",
      "        num_rows: 106438\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit'],\n",
      "        num_rows: 5649\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit'],\n",
      "        num_rows: 5788\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit'],\n",
      "        num_rows: 1242544\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit'],\n",
      "        num_rows: 67490\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit'],\n",
      "        num_rows: 67107\n",
      "    })\n",
      "})\n",
      "['normal: </s>haha you guys are a bunch of losers.', 'normal: </s>ur a sh*tty comment.', 'normal: </s>It\\'s ridiculous that these guys are being called \"protesters\". Being armed is a threat of violence, which makes them terrorists.', \"normal: </s>Yet call out all Muslims for the acts of a few will get you pilloried.   So why is it okay to smear an entire religion over these few idiots?  Or is this because it's okay to bash Christian sects?\", 'normal: </s>This bitch is nuts. Who would read a book by a woman.']\n",
      "[\"toxic: </s>This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!\", \"toxic: </s>Thank you!! This would make my life a lot less anxiety-inducing. Keep it up, and don't let anyone get in your way!\", 'toxic: </s>This is such an urgent design problem; kudos to you for taking it on. Very impressive!', \"toxic: </s>Is this something I'll be able to install on my site? When will you be releasing it?\", 'toxic: </s>FFFFUUUUUUUUUUUUUUU']\n"
     ]
    }
   ],
   "source": [
    "# split the training dataset in 2 sets based on toxicity level\n",
    "toxic_dataset = dataset.filter(lambda example: example['toxicity'] > 0.5)\n",
    "\n",
    "# for normal text toxicity, severe_toxicity, obscene, threat, insult, identity_attack, sexual_explicit are all 0\n",
    "def filter_normal(example):\n",
    "    if example['toxicity'] == 0 and example['severe_toxicity'] == 0 and example['obscene'] == 0 and example['threat'] == 0 and example['insult'] == 0 and example['identity_attack'] == 0 and example['sexual_explicit'] == 0:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "normal_dataset = dataset.filter(filter_normal)\n",
    "\n",
    "print(toxic_dataset)\n",
    "print(normal_dataset)\n",
    "\n",
    "toxic_dataset = toxic_dataset['train']\n",
    "normal_dataset = normal_dataset['train']\n",
    "\n",
    "# add prefixes normal: to the toxic dataset and toxic: to the normal dataset\n",
    "toxic_dataset = toxic_dataset.map(lambda example: {'text': 'normal: </s>' + example['text']})\n",
    "normal_dataset = normal_dataset.map(lambda example: {'text': 'toxic: </s>' + example['text']})\n",
    "\n",
    "print(toxic_dataset['text'][:5])\n",
    "print(normal_dataset['text'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit'],\n",
       "    num_rows: 106438\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit'],\n",
       "    num_rows: 1242544\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets, DatasetDict\n",
    "\n",
    "# toxic_dataset = toxic_dataset.select(range(100000))\n",
    "# normal_dataset = normal_dataset.select(range(100000))\n",
    "\n",
    "dataset = concatenate_datasets([toxic_dataset, normal_dataset])\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "# shuffle the dataset\n",
    "dataset = dataset.shuffle()\n",
    "\n",
    "#remove all columns except the text column\n",
    "dataset = dataset.remove_columns(['toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1214083\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 134899\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# split the dataset in 2 sets: train and test\n",
    "# train_dataset = dataset.train_test_split(test_size=0.2)\n",
    "# test_dataset = train_dataset['test']\n",
    "\n",
    "# print some information about the datasets\n",
    "print(dataset)\n",
    "# print(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (818 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "# load the T5 tokenizer, the T5 model \n",
    "tokenizer = AutoTokenizer.from_pretrained('ceshine/t5-paraphrase-paws-msrp-opinosis')\n",
    "\n",
    "def sort_dataset_by_difficulty(dataset):\n",
    "    # Compute lengths of sentences\n",
    "    lengths = [len(tokenizer.tokenize(example['text'])) for example in dataset]\n",
    "    sorted_indices = np.argsort(lengths)\n",
    "    \n",
    "    # Sort the dataset by sentence lengths\n",
    "    sorted_dataset = dataset.select(sorted_indices)\n",
    "    \n",
    "    return sorted_dataset\n",
    "\n",
    "toxic_dataset = sort_dataset_by_difficulty(toxic_dataset)\n",
    "normal_dataset = sort_dataset_by_difficulty(normal_dataset)\n",
    "# %%\n",
    "# add normal: and toxic: to the tokenizer as special tokens\n",
    "# tokenizer.add_special_tokens({'additional_special_tokens': ['translate Non-Toxic to Toxic:', 'translate Toxic to Non-Toxic:']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['normal: </s>Crazy',\n",
       " 'normal: </s>Bull crap',\n",
       " 'normal: </s>total garbage',\n",
       " 'normal: </s>kill her',\n",
       " 'normal: </s>Darn',\n",
       " 'normal: </s>loser',\n",
       " 'normal: </s>Your sick',\n",
       " 'normal: </s>Dumb',\n",
       " 'normal: </s>LIAR',\n",
       " 'normal: </s>You mad?']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_dataset['text'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7378bd48c98f4854a87e4db58f84c454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/27900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee6b30227a124311aea2d4b2fa161dbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define a function to tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, return_tensors='pt')\n",
    "\n",
    "# tokenize the train and test datasets\n",
    "tokenized_train_dataset = dataset['train'].map(tokenize_function, batched=True)\n",
    "tokenized_test_dataset = dataset['test'].map(tokenize_function, batched=True)\n",
    "\n",
    "# # remove the text column, and set format to pytorch\n",
    "# tokenized_train_dataset = tokenized_train_dataset.remove_columns(['text'])\n",
    "# tokenized_test_dataset = tokenized_test_dataset.remove_columns(['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 27900\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "def remove_short_sentence(ds):\n",
    "    valid_samples = []\n",
    "    for example in ds:\n",
    "        if len(example['text'].split()) > 5:\n",
    "            valid_samples.append(example)\n",
    "    del ds\n",
    "    dataset = Dataset.from_list(valid_samples)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "tokenized_train_dataset = remove_short_sentence(tokenized_train_dataset)\n",
    "tokenized_test_dataset = remove_short_sentence(tokenized_test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_train_dataset = tokenized_train_dataset\n",
    "# tokenized_test_dataset = tokenized_test_dataset\n",
    "dataset = DatasetDict({\n",
    "    'train': tokenized_train_dataset,\n",
    "    'test': tokenized_test_dataset\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['toxic: </s>Nice',\n",
       " 'toxic: </s>Testing',\n",
       " 'toxic: </s>nice',\n",
       " 'toxic: </s>100%',\n",
       " 'toxic: </s>nice',\n",
       " 'toxic: </s>Right',\n",
       " 'toxic: </s>No',\n",
       " 'toxic: </s>log',\n",
       " 'toxic: </s>Test',\n",
       " 'toxic: </s>First',\n",
       " 'toxic: </s>beautiful',\n",
       " 'toxic: </s>schools',\n",
       " 'toxic: </s>wrong',\n",
       " 'toxic: </s>boom',\n",
       " 'toxic: </s>nice',\n",
       " 'toxic: </s>Bingo',\n",
       " 'toxic: </s>nice',\n",
       " 'toxic: </s>log',\n",
       " 'toxic: </s>LOL',\n",
       " 'toxic: </s>wrong',\n",
       " 'toxic: </s>wrong',\n",
       " 'toxic: </s>37',\n",
       " 'toxic: </s>yes',\n",
       " 'toxic: </s>Good',\n",
       " 'toxic: </s>What',\n",
       " 'toxic: </s>OK',\n",
       " 'toxic: </s>LOL',\n",
       " 'toxic: </s>very',\n",
       " 'toxic: </s>are',\n",
       " 'toxic: </s>less',\n",
       " 'toxic: </s>yes',\n",
       " 'toxic: </s>No',\n",
       " 'toxic: </s>Congrats',\n",
       " 'toxic: </s>K',\n",
       " 'toxic: </s>Lucky',\n",
       " 'toxic: </s>me',\n",
       " 'toxic: </s>R',\n",
       " 'toxic: </s>indeed',\n",
       " 'toxic: </s>Solid',\n",
       " 'toxic: </s>gold',\n",
       " 'toxic: </s>hi',\n",
       " 'toxic: </s>Yes',\n",
       " 'toxic: </s>A',\n",
       " 'toxic: </s>No',\n",
       " 'toxic: </s>yes',\n",
       " 'toxic: </s>No',\n",
       " 'toxic: </s>Test',\n",
       " 'toxic: </s>good',\n",
       " 'toxic: </s>bull',\n",
       " 'toxic: </s>hi',\n",
       " 'toxic: </s>Absolutely',\n",
       " 'toxic: </s>Beautiful',\n",
       " 'toxic: </s>testing',\n",
       " 'toxic: </s>Thanks',\n",
       " 'toxic: </s>agreed',\n",
       " 'toxic: </s>Ben',\n",
       " 'toxic: </s>LOL',\n",
       " 'toxic: </s>LOL',\n",
       " 'toxic: </s>Yes',\n",
       " 'toxic: </s>nice',\n",
       " 'toxic: </s>Great',\n",
       " 'toxic: </s>good',\n",
       " 'toxic: </s>nice',\n",
       " 'toxic: </s>K',\n",
       " 'toxic: </s>Correct',\n",
       " 'toxic: </s>nice',\n",
       " 'toxic: </s>do',\n",
       " 'toxic: </s>Yes',\n",
       " 'toxic: </s>Interesting',\n",
       " 'toxic: </s>Wow',\n",
       " 'toxic: </s>drugs',\n",
       " 'toxic: </s>lol',\n",
       " 'toxic: </s>Yes',\n",
       " 'toxic: </s>Awesome',\n",
       " 'toxic: </s>Good',\n",
       " 'toxic: </s>nice',\n",
       " 'toxic: </s>Maybe',\n",
       " 'toxic: </s>Congrats',\n",
       " 'toxic: </s>Cool',\n",
       " 'toxic: </s>ignorance',\n",
       " 'toxic: </s>cool',\n",
       " 'toxic: </s>3',\n",
       " 'toxic: </s>cool',\n",
       " 'toxic: </s>Interesting',\n",
       " 'toxic: </s>No',\n",
       " 'toxic: </s>LOL',\n",
       " 'toxic: </s>Sad',\n",
       " 'toxic: </s>B',\n",
       " 'toxic: </s>LOL',\n",
       " 'toxic: </s>test',\n",
       " 'toxic: </s>Funny',\n",
       " 'toxic: </s>hi',\n",
       " 'toxic: </s>No',\n",
       " 'toxic: </s>agree',\n",
       " 'toxic: </s>nice',\n",
       " 'toxic: </s>Bravo',\n",
       " 'toxic: </s>nice',\n",
       " 'toxic: </s>True',\n",
       " 'toxic: </s>Good',\n",
       " 'toxic: </s>You',\n",
       " 'toxic: </s>deleted',\n",
       " 'toxic: </s>LOL',\n",
       " 'toxic: </s>LOL',\n",
       " 'toxic: </s>No',\n",
       " 'toxic: </s>Testing',\n",
       " 'toxic: </s>nice',\n",
       " 'toxic: </s>Absolutely',\n",
       " 'toxic: </s>log',\n",
       " 'toxic: </s>Yes',\n",
       " 'toxic: </s>Yum',\n",
       " 'toxic: </s>Yes',\n",
       " 'toxic: </s>Truth',\n",
       " 'toxic: </s>thanks',\n",
       " 'toxic: </s>Bingo',\n",
       " 'toxic: </s>2016',\n",
       " 'toxic: </s>Florida',\n",
       " 'toxic: </s>cool',\n",
       " 'toxic: </s>agreed',\n",
       " 'toxic: </s>Yes',\n",
       " 'toxic: </s>LOL',\n",
       " 'toxic: </s>GOOD',\n",
       " 'toxic: </s>no',\n",
       " 'toxic: </s>Whatever',\n",
       " 'toxic: </s>no',\n",
       " 'toxic: </s>nice',\n",
       " 'toxic: </s>Thanks',\n",
       " 'toxic: </s>nice',\n",
       " 'toxic: </s>nice',\n",
       " 'toxic: </s>5',\n",
       " 'toxic: </s>Awesome',\n",
       " 'toxic: </s>100%',\n",
       " 'toxic: </s>LOL',\n",
       " 'toxic: </s>yes',\n",
       " 'toxic: </s>log',\n",
       " 'toxic: </s>Cool',\n",
       " 'toxic: </s>Awesome',\n",
       " 'toxic: </s>lol',\n",
       " 'toxic: </s>yes',\n",
       " 'toxic: </s>No',\n",
       " 'toxic: </s>Interesting',\n",
       " 'toxic: </s>lol',\n",
       " 'toxic: </s>ALL',\n",
       " 'toxic: </s>yes',\n",
       " 'toxic: </s>LOL',\n",
       " 'toxic: </s>LOL',\n",
       " 'toxic: </s>yes',\n",
       " 'toxic: </s>false',\n",
       " 'toxic: </s>Plenty',\n",
       " 'toxic: </s>Wow',\n",
       " 'toxic: </s>yes',\n",
       " 'toxic: </s>Nice',\n",
       " 'toxic: </s>Awesome',\n",
       " 'toxic: </s>Bull',\n",
       " 'toxic: </s>Perfect',\n",
       " 'toxic: </s>LOL',\n",
       " 'toxic: </s>Baker Hughes',\n",
       " 'toxic: </s>Bingo....',\n",
       " 'toxic: </s>Awesome!',\n",
       " 'toxic: </s>why?',\n",
       " 'toxic: </s>🙂',\n",
       " 'toxic: </s>true!!',\n",
       " 'toxic: </s>Ha!',\n",
       " 'toxic: </s>Thank You',\n",
       " 'toxic: </s>totally irrelevant',\n",
       " 'toxic: </s>No.',\n",
       " 'toxic: </s>Amen',\n",
       " 'toxic: </s>thank you',\n",
       " 'toxic: </s>Why?',\n",
       " 'toxic: </s>Right?',\n",
       " 'toxic: </s>About time',\n",
       " 'toxic: </s>Player!',\n",
       " 'toxic: </s>Stockholm Syndrome',\n",
       " 'toxic: </s>CBC',\n",
       " 'toxic: </s>Get lost',\n",
       " 'toxic: </s>Eleven',\n",
       " 'toxic: </s>clearly.',\n",
       " 'toxic: </s>Yes!',\n",
       " 'toxic: </s>...',\n",
       " 'toxic: </s>amen',\n",
       " 'toxic: </s>Wow....',\n",
       " 'toxic: </s>*whether',\n",
       " 'toxic: </s>Democrats.',\n",
       " 'toxic: </s>Good.',\n",
       " 'toxic: </s>Amen',\n",
       " 'toxic: </s>RIP',\n",
       " 'toxic: </s>Who?',\n",
       " 'toxic: </s>Right!',\n",
       " 'toxic: </s>Wow.',\n",
       " 'toxic: </s>lovely.',\n",
       " 'toxic: </s>What?',\n",
       " 'toxic: </s>Congratulations!',\n",
       " 'toxic: </s>You first',\n",
       " 'toxic: </s>Bingo......',\n",
       " 'toxic: </s>I concur',\n",
       " 'toxic: </s>Absolutely!!!!!',\n",
       " 'toxic: </s>NICE',\n",
       " 'toxic: </s>Zika',\n",
       " 'toxic: </s>Wow!',\n",
       " 'toxic: </s>Rip',\n",
       " 'toxic: </s>or straight',\n",
       " 'toxic: </s>OMG',\n",
       " 'toxic: </s>Justice Alaska',\n",
       " 'toxic: </s>STEM?',\n",
       " 'toxic: </s>another test',\n",
       " 'toxic: </s>Wow.',\n",
       " 'toxic: </s>Maybe!!',\n",
       " 'toxic: </s>Amen',\n",
       " 'toxic: </s>Lol',\n",
       " 'toxic: </s>*cited',\n",
       " 'toxic: </s>LOL!',\n",
       " 'toxic: </s>Fantastic!',\n",
       " 'toxic: </s>RIP',\n",
       " 'toxic: </s>too late',\n",
       " 'toxic: </s>Nice job',\n",
       " 'toxic: </s>Brilliant!',\n",
       " 'toxic: </s>Thanks...',\n",
       " 'toxic: </s>No.',\n",
       " 'toxic: </s>Source?',\n",
       " 'toxic: </s>Nice.',\n",
       " 'toxic: </s>Truth!',\n",
       " 'toxic: </s>Thanks Alaska',\n",
       " 'toxic: </s>Epic!',\n",
       " 'toxic: </s>Plus margin',\n",
       " 'toxic: </s>Nice photo',\n",
       " 'toxic: </s>No.',\n",
       " 'toxic: </s>Meth',\n",
       " 'toxic: </s>Wow!',\n",
       " 'toxic: </s>UBER',\n",
       " 'toxic: </s>Yeah.',\n",
       " 'toxic: </s>Great!',\n",
       " 'toxic: </s>well done',\n",
       " 'toxic: </s>Well said',\n",
       " 'toxic: </s>Nope',\n",
       " 'toxic: </s>Who?',\n",
       " 'toxic: </s>👍🏼❤️',\n",
       " 'toxic: </s>It works',\n",
       " 'toxic: </s>Good.',\n",
       " 'toxic: </s>So?',\n",
       " 'toxic: </s>How?',\n",
       " 'toxic: </s>Yes.',\n",
       " 'toxic: </s>Amen',\n",
       " 'toxic: </s>Great story',\n",
       " 'toxic: </s>Of Venezuela',\n",
       " 'toxic: </s>strip it',\n",
       " 'toxic: </s>Finally.',\n",
       " 'toxic: </s>ok',\n",
       " 'toxic: </s>Why?',\n",
       " 'toxic: </s>Solid!',\n",
       " 'toxic: </s>waiting...',\n",
       " 'toxic: </s>Nice!!',\n",
       " 'toxic: </s>What?',\n",
       " 'toxic: </s>Absolutely agree',\n",
       " 'toxic: </s>selective?',\n",
       " 'toxic: </s>And so',\n",
       " 'toxic: </s>Bingo!!',\n",
       " 'toxic: </s>Wow!',\n",
       " 'toxic: </s>Diversity!',\n",
       " 'toxic: </s>semantics',\n",
       " 'toxic: </s>Sure...',\n",
       " 'toxic: </s>really?',\n",
       " 'toxic: </s>?',\n",
       " 'toxic: </s>POS',\n",
       " 'toxic: </s>Meds',\n",
       " 'toxic: </s>hilarious.',\n",
       " 'toxic: </s>Why?',\n",
       " 'toxic: </s>Cycling.',\n",
       " 'toxic: </s>purple?',\n",
       " 'toxic: </s>I agree',\n",
       " 'toxic: </s>No.',\n",
       " 'toxic: </s>Perfect!',\n",
       " 'toxic: </s>h',\n",
       " 'toxic: </s>No.',\n",
       " 'toxic: </s>Why?',\n",
       " 'toxic: </s>Well stated',\n",
       " 'toxic: </s>No.',\n",
       " 'toxic: </s>Excellent.',\n",
       " 'toxic: </s>Good.',\n",
       " 'toxic: </s>Beautiful!',\n",
       " 'toxic: </s>Nice field',\n",
       " 'toxic: </s>Awesome!',\n",
       " 'toxic: </s>Typical',\n",
       " 'toxic: </s>Yes.....',\n",
       " 'toxic: </s>?',\n",
       " 'toxic: </s>Wolf tickets',\n",
       " 'toxic: </s>Barf',\n",
       " 'toxic: </s>No....',\n",
       " 'toxic: </s>Toast.',\n",
       " 'toxic: </s>Well said',\n",
       " 'toxic: </s>wow',\n",
       " 'toxic: </s>Like that',\n",
       " 'toxic: </s>Wow.',\n",
       " 'toxic: </s>than*',\n",
       " 'toxic: </s>Sweet!',\n",
       " 'toxic: </s>Darwin award',\n",
       " 'toxic: </s>Perhaps!',\n",
       " 'toxic: </s>State run',\n",
       " 'toxic: </s>GOOD!',\n",
       " 'toxic: </s>Classic Alaska',\n",
       " 'toxic: </s>I concur',\n",
       " 'toxic: </s>Excellent.',\n",
       " 'toxic: </s>Why?',\n",
       " 'toxic: </s>nice article',\n",
       " 'toxic: </s>Google.',\n",
       " 'toxic: </s>Bravo!',\n",
       " 'toxic: </s>None.',\n",
       " 'toxic: </s>Good.',\n",
       " 'toxic: </s>Cheese?',\n",
       " 'toxic: </s>Amen',\n",
       " 'toxic: </s>👏👏👏👍👍👍',\n",
       " 'toxic: </s>Perfect!',\n",
       " 'toxic: </s>Awesome!',\n",
       " 'toxic: </s>exactly.',\n",
       " 'toxic: </s>Science!',\n",
       " 'toxic: </s>I concur',\n",
       " 'toxic: </s>Truth.',\n",
       " 'toxic: </s>LOL!',\n",
       " 'toxic: </s>No.',\n",
       " 'toxic: </s>natural selection',\n",
       " 'toxic: </s>please investigate',\n",
       " 'toxic: </s>Why?',\n",
       " 'toxic: </s>Bear.',\n",
       " 'toxic: </s>True.',\n",
       " 'toxic: </s>Good.',\n",
       " 'toxic: </s>?',\n",
       " 'toxic: </s>Yes.',\n",
       " 'toxic: </s>No.',\n",
       " 'toxic: </s>Cool!',\n",
       " 'toxic: </s>Bingo!!',\n",
       " 'toxic: </s>POS',\n",
       " 'toxic: </s>Lol',\n",
       " 'toxic: </s>Why?',\n",
       " 'toxic: </s>Cool!',\n",
       " 'toxic: </s>Absolutely!',\n",
       " 'toxic: </s>Perhaps!',\n",
       " 'toxic: </s>Good points',\n",
       " 'toxic: </s>Yes!',\n",
       " 'toxic: </s>Very cool',\n",
       " 'toxic: </s>?',\n",
       " 'toxic: </s>PDF?',\n",
       " 'toxic: </s>Who?',\n",
       " 'toxic: </s>Excellent!',\n",
       " 'toxic: </s>Yes!',\n",
       " 'toxic: </s>Nope',\n",
       " 'toxic: </s>Rip',\n",
       " 'toxic: </s>Winner!',\n",
       " 'toxic: </s>Nice.',\n",
       " 'toxic: </s>Cool.',\n",
       " 'toxic: </s>What?',\n",
       " 'toxic: </s>Beautiful!',\n",
       " 'toxic: </s>Who?',\n",
       " 'toxic: </s>Thanks AT',\n",
       " 'toxic: </s>Great story',\n",
       " 'toxic: </s>Thank you',\n",
       " 'toxic: </s>Source?',\n",
       " 'toxic: </s>Oh please',\n",
       " 'toxic: </s>SMH',\n",
       " 'toxic: </s>Haha',\n",
       " 'toxic: </s>Excellent.',\n",
       " 'toxic: </s>Touché',\n",
       " 'toxic: </s>precisely.',\n",
       " 'toxic: </s>never ever',\n",
       " 'toxic: </s>mi error',\n",
       " 'toxic: </s>No.',\n",
       " 'toxic: </s>100% incorrect',\n",
       " 'toxic: </s>source?',\n",
       " 'toxic: </s>Again.',\n",
       " 'toxic: </s>Why?',\n",
       " 'toxic: </s>LOL!',\n",
       " 'toxic: </s>homeless shelter',\n",
       " 'toxic: </s>Not.',\n",
       " 'toxic: </s>Epic!',\n",
       " 'toxic: </s>Lol',\n",
       " 'toxic: </s>Cool!',\n",
       " 'toxic: </s>What?',\n",
       " 'toxic: </s>40 years',\n",
       " 'toxic: </s>Thank you',\n",
       " 'toxic: </s>GOOD!',\n",
       " 'toxic: </s>Whatever.',\n",
       " 'toxic: </s>Amen',\n",
       " 'toxic: </s>Sad!',\n",
       " 'toxic: </s>LOL!',\n",
       " 'toxic: </s>Good.',\n",
       " 'toxic: </s>ho?',\n",
       " 'toxic: </s>Childish',\n",
       " 'toxic: </s>Thanks.',\n",
       " 'toxic: </s>Good.',\n",
       " 'toxic: </s>amen',\n",
       " 'toxic: </s>I do',\n",
       " 'toxic: </s>stampede',\n",
       " 'toxic: </s>Who?',\n",
       " 'toxic: </s>hey',\n",
       " 'toxic: </s>No.',\n",
       " 'toxic: </s>Bingo.',\n",
       " 'toxic: </s>Proof???',\n",
       " 'toxic: </s>I concur',\n",
       " 'toxic: </s>Cool.',\n",
       " 'toxic: </s>Sweet!',\n",
       " 'toxic: </s>Lol',\n",
       " 'toxic: </s>Nope',\n",
       " 'toxic: </s>Dream on',\n",
       " 'toxic: </s>great news',\n",
       " 'toxic: </s>Thank you',\n",
       " 'toxic: </s>Yes.',\n",
       " 'toxic: </s>got it',\n",
       " 'toxic: </s>Yep',\n",
       " 'toxic: </s>thank you',\n",
       " 'toxic: </s>Both?',\n",
       " 'toxic: </s>GOOD!',\n",
       " 'toxic: </s>Good.',\n",
       " 'toxic: </s>Amen',\n",
       " 'toxic: </s>Again.',\n",
       " 'toxic: </s>Results?',\n",
       " 'toxic: </s>Amen',\n",
       " 'toxic: </s>No...',\n",
       " 'toxic: </s>nope',\n",
       " 'toxic: </s>lol.',\n",
       " 'toxic: </s>Bingo.',\n",
       " 'toxic: </s>What?',\n",
       " 'toxic: </s>Truth!',\n",
       " 'toxic: </s>Truth!',\n",
       " 'toxic: </s>Whale.',\n",
       " 'toxic: </s>Bull!',\n",
       " 'toxic: </s>Beautiful!',\n",
       " 'toxic: </s>People.',\n",
       " 'toxic: </s>It works',\n",
       " 'toxic: </s>Sad story',\n",
       " 'toxic: </s>Well said',\n",
       " 'toxic: </s>Pray',\n",
       " 'toxic: </s>None.',\n",
       " 'toxic: </s>I agree',\n",
       " 'toxic: </s>Lisa!',\n",
       " 'toxic: </s>Rad.',\n",
       " 'toxic: </s>Excellent!',\n",
       " 'toxic: </s>For sure',\n",
       " 'toxic: </s>k',\n",
       " 'toxic: </s>Heroin',\n",
       " 'toxic: </s>Congrats!',\n",
       " 'toxic: </s>Pure fantasy',\n",
       " 'toxic: </s>Wow.',\n",
       " 'toxic: </s>Why?',\n",
       " 'toxic: </s>Perfect.',\n",
       " 'toxic: </s>FDR',\n",
       " 'toxic: </s>Excellent!',\n",
       " 'toxic: </s>Cha!',\n",
       " 'toxic: </s>2016 concerts',\n",
       " 'toxic: </s>For sure',\n",
       " 'toxic: </s>No.',\n",
       " 'toxic: </s>Absolutely!!!',\n",
       " 'toxic: </s>Good point',\n",
       " 'toxic: </s>what the',\n",
       " 'toxic: </s>spammer',\n",
       " 'toxic: </s>Bunk',\n",
       " 'toxic: </s>Poor folks',\n",
       " 'toxic: </s>Example?',\n",
       " 'toxic: </s>Respect.',\n",
       " 'toxic: </s>Test comment',\n",
       " 'toxic: </s>x',\n",
       " 'toxic: </s>WHAT?',\n",
       " 'toxic: </s>Wow.',\n",
       " 'toxic: </s>Touché',\n",
       " 'toxic: </s>Wat',\n",
       " 'toxic: </s>Lots?',\n",
       " 'toxic: </s>Good.',\n",
       " 'toxic: </s>LOL.',\n",
       " 'toxic: </s>Absolutely!',\n",
       " 'toxic: </s>Never!',\n",
       " 'toxic: </s>Wonderful!',\n",
       " 'toxic: </s>thanks...',\n",
       " 'toxic: </s>You start',\n",
       " 'toxic: </s>well good',\n",
       " 'toxic: </s>Excellent!',\n",
       " 'toxic: </s>Well said',\n",
       " 'toxic: </s>Thank you',\n",
       " 'toxic: </s>Barf',\n",
       " 'toxic: </s>Brilliant!!',\n",
       " 'toxic: </s>LOL!',\n",
       " 'toxic: </s>Why?',\n",
       " 'toxic: </s>Source??',\n",
       " 'toxic: </s>agree Sean',\n",
       " 'toxic: </s>Indeed.',\n",
       " 'toxic: </s>Where?',\n",
       " 'toxic: </s>Indeed.',\n",
       " 'toxic: </s>Wow!',\n",
       " 'toxic: </s>Excellent!',\n",
       " 'toxic: </s>Police?',\n",
       " 'toxic: </s>Excellent!',\n",
       " 'toxic: </s>RIP',\n",
       " 'toxic: </s>Funny.',\n",
       " 'toxic: </s>agreed.',\n",
       " 'toxic: </s>Amazing!!!',\n",
       " 'toxic: </s>Done',\n",
       " 'toxic: </s>Why?',\n",
       " 'toxic: </s>please read',\n",
       " 'toxic: </s>evidence please',\n",
       " 'toxic: </s>Yes.',\n",
       " 'toxic: </s>Dream on',\n",
       " 'toxic: </s>F Hillary',\n",
       " 'toxic: </s>Yes!',\n",
       " 'toxic: </s>Good!',\n",
       " 'toxic: </s>ugh']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_dataset['text'][0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 25340\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 2840\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3828"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "122519//64*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    }
   ],
   "source": [
    "dataset.save_to_disk('civil_comments_ds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12068, 10, 3, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1389, 10, 3, 1]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenizer.encode('toxic: </s>')[:-1])\n",
    "tokenizer.encode('normal: </s>')[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1389, 10, 3, 1, 12336]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "style_code = tokenizer.encode(dataset['train'][0]['text'])[:5]\n",
    "style_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁non', '-', 'toxic', ':', '▁', '</s>', '</s>']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenizer.encode('non-toxic: </s>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12068, 10, 1, 1]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('toxic:</s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [12068, 10, 1], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('toxic:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "style_code == tokenizer.encode('toxic:')[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.where(style_code == tokenizer.encode('toxic:'), tokenizer.encode('normal:'), tokenizer.encode('toxic:'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = 0\n",
    "for i in range(50):\n",
    "    if (dataset['train']['text'][i][:6].startswith('normal')):\n",
    "        c+=1\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Data/deeksha/anaconda3/envs/pssp_main/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset civil_comments (/Data/deeksha/.cache/huggingface/datasets/civil_comments/default/0.9.0/e7a3aacd2ab7d135fa958e7209d10b1fa03807d44c486e3c34897aa08ea8ffab)\n",
      "100%|██████████| 3/3 [00:00<00:00, 33.87it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"civil_comments\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  15%|█▍        | 266827/1804874 [00:28<02:25, 10541.79 examples/s]"
     ]
    }
   ],
   "source": [
    "# Check the distribution of toxic and non-toxic comments\n",
    "# toxic_count = len([x for x in dataset[\"train\"][\"toxicity\"] if x > 0.8])\n",
    "# non_toxic_count = len(dataset[\"train\"][\"toxicity\"]) - toxic_count\n",
    "\n",
    "# print(f\"Toxic Comments: {toxic_count}\")\n",
    "# print(f\"Non-Toxic Comments: {non_toxic_count}\")\n",
    "\n",
    "# Convert toxicity scores to binary labels (0 for non-toxic, 1 for toxic)\n",
    "def mark_sample(example):\n",
    "    example.setdefault('label', None)\n",
    "    if example[\"toxicity\"] > 0.8:\n",
    "        example[\"label\"] = 1\n",
    "    elif example['toxicity'] == 0 and example['severe_toxicity'] == 0 and example['obscene'] == 0 and example['threat'] == 0 and example['insult'] == 0 and example['identity_attack'] == 0 and example['sexual_explicit'] == 0:\n",
    "        example[\"label\"] = 0\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(mark_sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the training dataset based on comment length\n",
    "sorted_dataset = dataset[\"train\"].sort('text')\n",
    "\n",
    "# Filter out comments with fewer than 4 tokens\n",
    "filtered_dataset = sorted_dataset.filter(lambda x: len(x[\"text\"].split()) >= 4)\n",
    "\n",
    "# Decide on the size of the subset\n",
    "subset_size = 1  # Example size, can be adjusted\n",
    "\n",
    "# Sample an equal number of toxic and non-toxic comments\n",
    "toxic_samples = filtered_dataset.filter(lambda x: x[\"label\"] == 1).select(range(subset_size // 2))\n",
    "non_toxic_samples = filtered_dataset.filter(lambda x: x[\"label\"] == 0).select(range(subset_size // 2))\n",
    "\n",
    "# Combine the samples to create the balanced subset\n",
    "balanced_subset = concatenate_datasets([toxic_samples, non_toxic_samples])\n",
    "\n",
    "balanced_subset.save_to_disk(\"./civil_comments_subset\")\n",
    "\n",
    "print(balanced_subset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
